{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLG454E - Learning From Data, Homework 3\n",
    "\n",
    "In this homework, you are supposed to implement following parts:\n",
    "\n",
    " -  **Part 1: Soft-margin SVM loss and one-versus-all classification (40 points)**\n",
    "     - Refer to Machine Learning Blinks 8 and 9 for this part.\n",
    "     - ML Blinks 8: https://www.youtube.com/watch?v=KWp_TxnWfSU&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ\n",
    "     - ML Blinks 9: https://www.youtube.com/watch?v=7bIVFiKpMfg&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ\n",
    "     \n",
    "     \n",
    " - **Part 2: practice dimensionality reduction (20 points)**\n",
    "     - Refer to Machine Learning Blinks 10 and 11 for this part:\n",
    "     - ML Blinks 10: https://www.youtube.com/watch?v=laeth5oT9YM&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ\n",
    "     - ML Blinks 11: https://www.youtube.com/watch?v=mRmVKNklE9I&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ\n",
    "     \n",
    "     \n",
    " - **Part 3: solve an SVM optimization problem by hand (40 points)**\n",
    " \n",
    " \n",
    " ### Important Notes:\n",
    "   - Please complete this template and include any other necessary materials (screenshots of your handwritten solutions etc.) into the HW3 folder. Then zip it again and submit to Ninova.\n",
    "   - For Part 1, you need to implement the required functions and gradients etc. by yourself. Do not use autograd or any built-in functions.\n",
    "   - For the Part 2, you can use scikit-learn built-in functions for training a learner, feature selector and PCA. \n",
    "   - At Part 3, you can upload the screenshots of your handwritten solutions to the Notebook. But please be sure that your solutions are neat and can be read properly.\n",
    "   - You can ask your questions on Ninova message board or send an e-mail to akti15@itu.edu.tr.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is associated with the book\n",
    "# \"Machine Learning Refined\", Cambridge University Press, 2016.\n",
    "# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Code up gradient descent for the softmax approximation of SVM loss (20 points)\n",
    "\n",
    "We have seen the margin perceptron and its implementation at previous lectures. SVM loss also uses the margin loss as base and tries to define and maximize a margin between classes as the following figure shows.\n",
    "\n",
    "\n",
    "<img src=\"images/part1-1.png\" width = \"450\">\n",
    "\n",
    "\n",
    "The soft-margin SVM loss with regularization can be calculated using following equation.\n",
    "\n",
    "<br>\n",
    "<center> $g(b, w) = \\sum_{p=1}^{P} max(0, 1 - y_{p}(b + x_{p}^T w)) + \\lambda\\left \\| w \\right \\|_2^2$ </center>\n",
    "<br>\n",
    "\n",
    "We can write this loss formula using the softmax approximation as follows:\n",
    "\n",
    "\n",
    "<br>\n",
    "<center> $g(b, w) = \\sum_{p=1}^{P} log ( 1 + e^{- y_{p}(b + x_{p}^T w)}) + \\lambda\\left \\| w \\right \\|_2^2$ </center>\n",
    "    \n",
    "<br>   \n",
    "\n",
    "Please note that the following template creates w and x vectors as $ \\tilde{x}_{p} = \\begin{bmatrix} 1\\\\x_{p}\\end{bmatrix}^T$ and $\\tilde{w} = \\begin{bmatrix}b\\\\w \\end{bmatrix} $\n",
    "\n",
    "We will use gradient descent to minimize softmax approximation of soft-margin SVM loss in order to fit a linear classifier on given overlapping 2-class dataset.\n",
    "\n",
    "Following function loads data, creates X and y matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data \n",
    "def load_data(csvname):\n",
    "    # load in data\n",
    "    data = np.asarray(pd.read_csv(csvname,header = None))\n",
    "\n",
    "    X = data[:,0:-1]\n",
    "    y = data[:,-1]\n",
    "    y.shape = (len(y),1)\n",
    "    \n",
    "    # pad data with ones for more compact gradient computation\n",
    "    o = np.ones((np.shape(X)[0],1))\n",
    "    X = np.concatenate((o,X),axis = 1)\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** The following function finds the optimal w values for the given input. The parameters of the function are samples in X, labels in y, initial w value, learning rate alpha and the regularization parameter $\\lambda$. You need to calculate the gradient of the given **softmax approximated SVM loss with regularization**  function and apply gradient descent (you can use Vanilla gradient descent here). **Do not forget to add the regularization term into gradient and remind that bias term should not be included in regularization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: YOUR CODE GOES HERE ###\n",
    "# run gradient descent\n",
    "def gradient_descent_soft_cost(X,y,w,alpha,lam):\n",
    "    # start gradient descent loop\n",
    "    max_its = 10000\n",
    "    for k in range(max_its):\n",
    "        \n",
    "        # fill here\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** The following plot function plots the data points and the separating line. Here, you need to add $x_1$ and $x_2$ lines to the plot in order to see the margins as shown in Fig 4.14 above. Please show the margins with dashed lines in order to get a better visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: YOUR CODE GOES HERE ###\n",
    "# plotting functions\n",
    "def plot_all(X,y,w,lam):\n",
    "    # custom colors for plotting points\n",
    "    red = [1,0,0.4]  \n",
    "    blue = [0,0.4,1]\n",
    "    \n",
    "    # scatter plot points\n",
    "    fig = plt.figure(figsize = (4,4))\n",
    "    ind = np.argwhere(y==1)\n",
    "    ind = [s[0] for s in ind]\n",
    "    plt.scatter(X[ind,1],X[ind,2],color = red,edgecolor = 'k',s = 25)\n",
    "    ind = np.argwhere(y==-1)\n",
    "    ind = [s[0] for s in ind]\n",
    "    plt.scatter(X[ind,1],X[ind,2],color = blue,edgecolor = 'k',s = 25)\n",
    "    plt.grid('off')\n",
    "    \n",
    "    # plot separator\n",
    "    s = np.linspace(-1,1,100) \n",
    "    plt.plot(s,(-w[0]-w[1]*s)/w[2],color = 'k',linewidth = 2)\n",
    "    \n",
    "    #### HINT ####\n",
    "    # The above line is b + X.T w = 0 or ax + by + c = 0 as a is w[1], b is w[2] and c is w[0]\n",
    "    # It is written in form of y = ( - c - ax )/b\n",
    "    # You need to plot b + X.T w = +1 and b + X.T w = -1 which means ax + by + c = +1 and ax + by + c = -1\n",
    "    \n",
    "    # Plot the line equations with following 2 lines of code\n",
    "    \n",
    "    plt.plot(...)\n",
    "    plt.plot(...)\n",
    "\n",
    "    # clean up plot\n",
    "    plt.xlim([-0.1,1.1])\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.title('soft-margin svm with lambda = ' + str(lam))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell you need to run 5 experiments which are:\n",
    "   - No regularization\n",
    "   - Regularization with $\\lambda = 10^{-2}$\n",
    "   - Regularization with $\\lambda = 10^{-1}$\n",
    "   - Regularization with $\\lambda = 1$\n",
    "   - Regularization with $\\lambda = 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X,y = load_data('overlapping_2class.csv')\n",
    "\n",
    "alpha = 10**(-2)\n",
    "w0 = np.random.randn(3,1)\n",
    "\n",
    "lams = [0, 10**-2, 10**-1, 1, 10]\n",
    "for lam in lams:\n",
    "    # run gradient descent\n",
    "    w = gradient_descent_soft_cost(X,y,w0,alpha,lam)\n",
    "\n",
    "    # plot points and separator\n",
    "    plot_all(X,y,w,lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Answer the following questions.\n",
    " - What is the effect of regularization and lambda value on learner?\n",
    " - Briefly explain your experiment results. What is your observation on the best lambda value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click here to insert your answer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Code up gradient descent for one-versus-all classifier (20 points)\n",
    "\n",
    "In this part, you are supposed to apply classification on a toy dataset which contains $C = 4$ classes.\n",
    "\n",
    "<img src=\"images/part1-2.png\" width = \"400\">\n",
    "\n",
    "As we have seen in the lectures, for OvA (One-versus-all) classification, we need to train 1 classifier for each class and then combine their outputs to get a single classifier that can separate all classes. For this experiment, you can use the **gradient descent for softmax loss** implementation that we have done at Tutorial 3, there can be minor changes needed due to matrix shape changes. **(4 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: YOUR CODE GOES HERE ###\n",
    "# gradient descent function for softmax cost/logistic regression \n",
    "def softmax_grad(X,y):\n",
    "    # Initializations \n",
    "    w = np.random.randn(3,1)      # random initial point\n",
    "    alpha = 10**-2\n",
    "    max_its = 2000\n",
    "    \n",
    "    for k in range(max_its):\n",
    "        \n",
    "        ## fill here \n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(8 points)** In the following cell, you need to train 1 classifier for each class. While sending the samples and labels into the optimizer, you need to assign the label for class C as -1 and the labels for all other classes should be assigned as 1 temporarily since we are doing one-versus-all classification. We keep the weight vectors for each class in W matrix in order to use them later for evaluating the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: COMPLETE ONE VERSUS ALL SNIPPET BELOW ###\n",
    "# learn all C separators\n",
    "def learn_separators(X,y):\n",
    "    W = []\n",
    "    classes = np.unique(y)\n",
    "    for c in classes:\n",
    "        \n",
    "        # fill here\n",
    "        \n",
    "        W.append(w)\n",
    "    W = np.asarray(W)\n",
    "    W.shape = (np.shape(W)[0],np.shape(W)[1])\n",
    "    W = W.T\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(8 points)** Now, using the weight vectors we determined, we need to classify the samples in given matrices and calculate the overall accuracy score. In order to decide the class of a given sample using OvA classifier, we can use following formula:\n",
    "\n",
    "<br>\n",
    "<center> $y = \\underset{j = 1, 2, ..., C}{argmax} \\ b_j + x^T w_j$ </center>\n",
    "<br>\n",
    "\n",
    "\n",
    "**Please do not loop over all samples but use the vectorized calculations.**\n",
    "\n",
    "After predicting the labels for all samples in given set, we can calculate the accuracy as:\n",
    "\n",
    "<br>\n",
    "<center> $ Accuracy = \\frac{\\#correct \\ predictions}{\\#total \\ samples} $ </center>\n",
    "<br>\n",
    "\n",
    "Please be careful about the sizes of y and y_pred vectors and remind that the class numbers are starting from 1, not 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: COMPLETE ACCURACY SCORE FUNCTION BELOW ###\n",
    "\n",
    "def accuracy_score(X, y, W):\n",
    "    \n",
    "    # fill here\n",
    "    \n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data, separators, and fused fule\n",
    "def plot_all(X,y,W):\n",
    "    # initialize figure, plot data, and dress up panels with axes labels etc.\n",
    "    X = X.T\n",
    "    num_classes = np.size(np.unique(y))\n",
    "    color_opts = np.array([[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.7, 0.6, 0.5]])\n",
    "    f,axs = plt.subplots(1,3,facecolor = 'white',figsize = (10,3))\n",
    "    for a in range(0,3):\n",
    "        for i in range(0,num_classes):\n",
    "            s = np.argwhere(y == i+1)\n",
    "            s = s[:,0]\n",
    "            axs[a].scatter(X[1,s],X[2,s], s = 30,color = color_opts[i,:])\n",
    "\n",
    "        # dress panel correctly\n",
    "        axs[a].set_xlim(0,1)\n",
    "        axs[a].set_ylim(0,1)\n",
    "        axs[a].axis('off')\n",
    "\n",
    "    r = np.linspace(0,1,150)\n",
    "    for i in range(0,num_classes):\n",
    "        z = -W[0,i]/W[2,i] - W[1,i]/W[2,i]*r\n",
    "        axs[1].plot(r,z,'-k',linewidth = 2,color = color_opts[i,:])\n",
    "\n",
    "    # fuse individual subproblem separators into one joint rule\n",
    "    r = np.linspace(0,1,300)\n",
    "    s,t = np.meshgrid(r,r)\n",
    "    s = np.reshape(s,(np.size(s),1))\n",
    "    t = np.reshape(t,(np.size(t),1))\n",
    "    h = np.concatenate((np.ones((np.size(s),1)),s,t),1)\n",
    "    f = np.dot(W.T,h.T)\n",
    "    z = np.argmax(f,0)\n",
    "    f.shape = (np.size(f),1)\n",
    "    s.shape = (np.size(r),np.size(r))\n",
    "    t.shape = (np.size(r),np.size(r))\n",
    "    z.shape = (np.size(r),np.size(r))\n",
    "\n",
    "    for i in range(0,num_classes + 1):\n",
    "        axs[2].contour(s,t,z,num_classes-1,colors = 'k',linewidths = 2.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train and test our learner. At the end, you should get at least 70% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ,y  = load_data('four_class_data.csv')\n",
    "\n",
    "# learn all C vs notC separators\n",
    "W = learn_separators(X,y)\n",
    "\n",
    "# calculate accuracies for both training and testing sets\n",
    "accuracy = accuracy_score(X, y, W)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# plot data and each subproblem 2-class separator\n",
    "plot_all(X, y, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature selection using scikit-learn (20 points)\n",
    "\n",
    "In this part, we will use scikit-learn library. You can install the necessary package using following commands:\n",
    "\n",
    "        > python3 -m pip install scikit-learn\n",
    "        > conda install -c conda-forge scikit-learn\n",
    "        \n",
    "There are lots of machine learning techniques that are available in scikit-learn library. In this problem we will use the **SVM** classifier with linear kernel and:\n",
    "   \n",
    " **1)** Apply feature selection on data using Variance Threshold and examine the results.\n",
    "    \n",
    " **2)** Apply dimensionality reduction with PCA to plot the data on 2D space.\n",
    "\n",
    "You can check the documentations on the internet to learn how to use these functions and which parameters to use. Necessary functions are imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.datasets as ds\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement feature selection on handwritten digits dataset. In the following cell we load and examine dataset. As you can see the samples are 8 by 8 images where in each row of X, the images are kept as flatten. Each feature represents a pixel (i.e 0th feature is (0, 0) pixel and 8th feature is (1, 0) pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_data = ds.load_digits()\n",
    "print(\"Number of samples: \", digit_data.data.shape[0])\n",
    "print(\"Number of attributes: \", digit_data.data.shape[1])\n",
    "print(\"Classes: \", digit_data.target_names)\n",
    "\n",
    "c = digit_data.images[0]\n",
    "for i in range(1, 10):\n",
    "    c = np.concatenate((c, digit_data.images[i]), 1)\n",
    "\n",
    "plt.gray() \n",
    "plt.matshow(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the data into training and testing sets in order to train the models on training set and test them on unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = digit_data.data, digit_data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True, random_state=20)\n",
    "\n",
    "print(\"Number of training samples: \", X_train.shape[0])\n",
    "print(\"Number of testing samples: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, you are given the necessary train and test functions, you can use them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y):\n",
    "    \n",
    "    classifier = SVC(gamma=\"auto\", kernel=\"linear\")\n",
    "    classifier = classifier.fit(X,y)\n",
    "    \n",
    "    preds = classifier.predict(X)\n",
    "    \n",
    "    train_accuracy = np.mean(preds==y)\n",
    "    \n",
    "    return classifier, train_accuracy\n",
    "\n",
    "def test(classifier, X, y):\n",
    "    \n",
    "    test_accuracy = np.mean(classifier.predict(X) == y)\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's train an SVM on the whole data and print the accuracies on training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier without feature selection\n",
    "svm, train_acc = train(X_train, y_train)\n",
    "test_acc = test(svm, X_test, y_test)\n",
    "\n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Now, fit a Variance Threshold feature selector on data. Print the number of selected features and the accuracies on training and test sets. Check the selected features. (You can use methods of VarianceThreshold to get the selected feature indexes.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: YOUR CODE GOES HERE##\n",
    "\n",
    "# SVM classifier with Variance Threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points) Question:**\n",
    "- Compare the results of training with and without feature selection, what is your observation?\n",
    "- Which features are eliminated? Comment on them by referring the method and the images displayed at load data cell.\n",
    "- What type of feature selector the Variance Threshold is? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click here to insert your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** We cannot visualize the data in 2D space since the feature space is high dimensional. So, we need to reduce it into 2 dimensions to plot them. In the following cell, use PCA to reduce the dimension of data and then plot the data samples in a 2D plot as each class is visualized with a different color. Then, simply train a classifier on reduced data to evaluate it. Print the accuracies on train and test sets again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: YOUR CODE GOES HERE##\n",
    "\n",
    "# PCA for dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points) Question:**\n",
    "\n",
    "- What do you think about the data distribution considering evaluation? Is 2 dimensions are enough to represent this data? \n",
    "- Both Variance Threshold and PCA are used for dimensionality reduction and reduced the size of feature space. What is the main difference between these two methods based on types of dimensionality reduction? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click here to insert your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Solving SVM optimization by hand (40 points)\n",
    "\n",
    "You can insert the screenshots of your handwritten solution on Jupyter Notebook. For an example, check the cells including images. Do not forget to include your solution image file into the submitted .zip file.\n",
    "\n",
    "Some reminders for the question:\n",
    "\n",
    " - Lagrangian to optimize: $\\mathcal{L}_{primal} = \\sum_{i=1}^{n} a_{i} - \\frac{1}{2} [\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x^{j}] $ \n",
    "\n",
    "\n",
    "- Constraint: $\\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0$\n",
    "\n",
    "\n",
    "- Optimal parameter: $w^{*} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} x^{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1 (20 points)\n",
    "\n",
    "<p style=\"float: left;\"><img src=\"images/part3-1.png\" width = \"200\"></p>\n",
    "        \n",
    "        Given the two following training samples (n=2), provide below a step-by-step solution\n",
    "        to estimate the optimal parameters (w and b) of the hyperplane separating the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2 (10 points) \n",
    "If we add a third training point $x_3 = \\left [\\begin{matrix} -1 \\\\ 0 \\end{matrix}\\right] $, will that impact the hyperplane estimated using points $x_1$ and $x_2$? Answer this considering two cases where label for $x_3$ is (1) positive, (2) negative and justify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.3 (10 points)\n",
    "Explain how to classify the point $x_{test} = \\left [\\begin{matrix} 1 \\\\ -4 \\end{matrix}\\right] $ using the estimated model. What is the predicted label of $x^{test}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.3 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
